---
title: "Clustering with mixture model"
author: "Boubacar Sow"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document: default
vignette: >
  %\VignetteIndexEntry{em-mixture-models}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

# Introduction 
Clustering analysis in unsupervised learning consist of finding partitions with the same structures in the data. In this context, the labels of the data are unknown as opposed to a classification with known labels. There are several techniques in machine learning designed to perform clustering with unknown labels, we can mention few of them  like k-means, autoencoders, mixture models, etc. 

I implemented a in R package that I called **mixVarClust**  a clustering technique using finite mixture models. The purpose of this post is to provide a documentation on the models and algorithms implemented in the package.  In the following lines of this post,  I present the general case of the finite mixture models in section 1. In the sub-sections,  I gave the definition of the model, then explain how to estimate  it parameters using the EM algorithm, and how to retrieve the clusters sought. In the section 2,  I introduced the well known gaussian mixture model, followed by the  multinomial miture model in section 3. and the gaussian-multinomial mixture model in section 3.

# The finite mixture model
Let $X \in \mathbb{R}^d$ be a dataset composed by $n$ observations of dimension $d \ge 1$. We suppose that $X$ is composed by $K \in \mathbb{N}$ different clusters with the same structure and each cluster has its own probability distribution parameterized with different set of parameters. A finite mixture model represent the probabiliy distribution of the observation $x_i$ parameterized by $\Theta$, formally : 

\begin{equation}
f_X(\mathbf{x_i}) = \sum_{k=1}^{K}p_kf_k(x_i|\theta_k)
\label{mixmod}
\end{equation}

where :  

  * $x_i= (x_{i,1},..., x_{i,d}) \in X$ is a single observation. $X$ is $n \times d$ matrix or a data table.  
  * $p_k$ is the proportion of each group, also called mixing proportion. It is the probability that an observation was generated by the $k$th group : $$p_k = Prob(z_{ik} = 1),$$ where $k \in K$,  $0<p_k<1$ and $\sum_kp_k=1$. $z_{ik}$ is the latent variable in the clustering context. 
  * $\Theta = (p_1, ..., p_K, \theta_1, ..., \theta_K)$ is the set of parameters of the marginal distribution (or the full mixture). 
 Throughout this post, we will keep these notations and their definitions.
  * $f_k(x_i|\theta_k)$ is the probability distribution of $x_i$ parameterize by $\theta_k$ in the $k$th component of the mixture. $\theta_k$ is the set of parameters of the density or probability mass function of $f_k$.   

$f_k(.|\theta_k)$ can be any distribution. For instance, in the case of only continuous variables in the studied dataset, $f_k$ might be a multivariate  or univariate gaussian distribution (in this case $\theta_k = (\mu_k, \Sigma_k)$) or a student distribution, etc. If the dataset is composed by categorical variables, $f_k$ is usually the multinomial distribution (when there are multiple different categories) or binomial distribution (when we have binary variables). For mixed variables, continuous and categorical, $f_k$ is composed by multiple different distributions suitable to each variable. 

## Estimation of a finite mixture model with unknown labels and the EM algorithm
### Maximum likelihood estimation (MLE)
The parameters of a  mixture model with known labels is estimated by the maximum likelihood. But in an unsupervised setting, we can show that, it is not possible to use the MLE directly. The likelihood of the mixture model with unknown labels is defined as : 
$$
L\left(\Theta;x_{i}\right)=\prod_{i=1}^{n} f_{X}\left(x_{i} \mid \Theta\right)=\prod_{i=1}^{n} \sum_{k} p_kf_{k}\left(x_{i} \mid \theta_{k}\right)\\
$$
For convinience we try to maximize the log-likelihood function, also called *incomplete log-likelihood* (because the labels are not observed): 

$$
\begin{align}
\ell(\Theta; x_{i}) &= \log L(\Theta ; x_{i}) \notag \\
\ell\left(\Theta; x_{i}\right)&=\log \left[\prod_{i=1}^{N} \sum_{k=1}^{K} p_{k} f_{k}\left(x_{i} \mid \theta_{k}\right)\right] \notag \\
\ell\left(\Theta; x_{i}\right)&=\sum_{i=1}^{N} \log \left(\sum_{k=1}^{K} p_{k} f_{k}\left(x_{i} \mid \theta_{k}\right)\right)
\label{incomplik}
\end{align}
$$
To find the maximum likelihood estimates, we need to maximize $\ell\left(\Theta; x_{i}\right)$ with respect to $\Theta$, which implies the derivation of the  $\ell\left(\Theta; x_{i}\right)$ with respect to $p_k$ and $\theta_k$. But as one can see in the equation $\ref{incomplik}$, the parameters of the log-likelihood function $\ell\left(\Theta; x_{i}\right)$ cannot be derived, formally because we have a log of sum. This computation issue is caused by the ignorance of the labels $\mathbf{z}$.  In contrast, if $\mathbf{z}$ were known, namely in a classification context, we would not have this problem. In that case, we can show that the log-likelihood called *complete likelihood* is defined as : 
  $$\begin{equation}
  \ell_c(\Theta; \mathbf{x_i}, z_{i}) = \sum_i^n \sum_k^K \tilde{z}_{ik}\log \big[p_kf_k(\mathbf{x_i}|\theta_k)\big] \label{complik} \end{equation},  $$  

However, the parameters in $\ell\left(\Theta; x_{i}\right)$ can be approximated using the EM algorithm. Next, we show how this algorithm make use of the complete log-liklihood (eq. $\ref{complik}$) to approximate the parameters of the incomplete log likelihood. 

## The EM algorithm  
| The *Expected-maximization* (EM) algorithm  is an iterative method to find a local maximum likelihood and a maximum a posteriori (MAP) estimates of parameters in the case of unobserved labels, unknown parameters and known data. The algorithm consist of alternating two steps called E-Step and M-Step (the **E** stands for Expectation and the **M** for maximization).  

|    **E-step**:    

|        First we estimate the incomplete log-likelihood $\ell\left(\Theta; x_{i}\right)$ by computing the expectation of the complete likelihood (equation $\ref{complik}$). Let $q \in \mathbb{N}$ be the $q$th iteration and the function $Q$ be the expectation of $\ell_c$, formally :

$$
Q\left(\Theta, \Theta^{q}\right)=E\left[\ell_{c}\left(\Theta; x_{i}, z_i\right) | x_{i}, \Theta^{q}\right] =\sum_i^N \sum_k^K  E\left[\tilde{z}_{i k}|{\mathbf{x_i}}, \boldsymbol{\Theta}^{q}\right] \log\big[p_kf_k(x_i∣\theta_k)\big],
$$

|         where:
$$
\begin{align}
E\left[\tilde{z}_{i k}|{\mathbf{x_i}}, \boldsymbol{\Theta}^{q}\right] &=1 \times P\left(\tilde{z}_{i k}=1 \mid \mathbf{x}_{i}, \boldsymbol{\theta}^{q}\right)+0 \times P\left(\tilde{z}_{i k}=0 \mid \mathbf{x}_{i}, \boldsymbol{\theta}^{q}\right) \notag \\
&=\frac{f_{\mid \tilde{z}_{i k}=1}\left(\mathbf{x}_{i}| \boldsymbol{\theta}^{q}\right) P\left(\tilde{z}_{ik}=1 \mid \boldsymbol{\theta}^{q}\right)}{f_X\left(\mathbf{x}_{i}| \boldsymbol{\Theta}^{q}\right)} \notag \\
&=\frac{f_{k}\left(\mathbf{x}_{i}| \boldsymbol{\theta}^{q}\right) p_{k}^{q}}{f_{\mathbf{X}}\left(\mathbf{x}_{i}|\Theta^{q}\right)} \notag \\
&=t_{k}^{q}\left(\mathbf{x}_{i}|\mathbf{\Theta}^q\right) \notag
\end{align} 
$$

To simplify notation let denote $t_{ik}^q =t_{k}^{q}\left(\mathbf{x}_{i}|\mathbf{\Theta}^q\right)$, the expection of the complete log-likelihood become : 

\begin{equation}
Q\left(\Theta, \Theta^{q}\right)=\sum_i^N \sum_k^K \mathbf{t_{ik}^{q} }\log\big[p_kf_k(x_i∣\theta_k)\big]
\label{expectation}
\end{equation}
$t_{ik}^q$ is called the maximum a posteriori (MAP), it is the probability that the observation $x_i$ belongs to the $k$th cluster. In practice, only the MAP is computed in the E Step. At the *first iteration* ($q = 0$) of the algorithm, $t_{ik}^0$  is computed using arbitrary initial parameters ($\Theta^0$). In the second and following iterations, we compute the MAP using the paramters $\Theta^q$ found in the M-step at the $q$th iteration. 

$$
t_{k}^{q}\left(\mathbf{x}_{i} \mid \Theta^{q}\right)=\frac{p_{k}^{q} f_k\left(\mathbf{x}_{i} \mid \boldsymbol{\theta}_{k}^{q}\right)}{\sum_{\ell=1}^{K} p_{\ell}^{q} f_\ell\left(\mathbf{x}_{i} \mid \boldsymbol{\theta}_{\ell}^{q}\right)}
$$

|    __M-step__  
In this step we maximise the expectation $Q$ with respect $\Theta = (p_k, \theta_k)$. In another words, we find new estimates of the parameters $\boldsymbol{\Theta}^{q+1}$ that maximize the expected complete log-likelihood (equation $\ref{expectation}$) : 

$$
\begin{equation}
\boldsymbol{\Theta}^{q+1}=\underset{\boldsymbol{\Theta}}{\operatorname{argmax}} Q\left(\boldsymbol{\Theta}, \boldsymbol{\Theta}^{q}\right)
\label{max}
\end{equation}
$$
Maximimising $Q$, involve deriving the equation $\ref{expectation}$ with respect to the parameters, and subject to $\sum_kp_k=1$. Additional constraints on the objective function $Q$ maybe necessary depending on the probability distribution of $f_k$. Using the lagrangian, we can derive the estimates $\hat{p}_k$ and $\hat{\theta}_k$.

|        Update the parameters ; 
|        Use a stopping rule to stop the iterations.

#### Stopping rules used in *mixVarClust*
There are many ways to stop the EM algorithm. I choose to implement the 2 following rules in my package *mixVarClust*. The first stopping rule is the convergence of the incomplete likelihood, which is computed by using the new found parameters $$|\ell(x, \Theta^{q+1}) -\ell(x, \Theta^{q})|<\epsilon. $$ The second rule implemented,  is a arbitrary pre-defined number of iterations of the algorithm. 

#### Initialization strategy in *mixVarClust* :
There are many ways to choose the initial parameters of the EM algorithm. In *mixVarClust*, I initialize randomly the vector $z$, then I compute the estimates $\Theta^0$ and the log-likelihood (equation $\ref{incomplik}$). I repeat this process *multiple times*, then I choose as initial parameters, the values that give the highest log-likelihood. 

## Retrieving the found clusters
The end goal of clustering  is to find the labels $\mathbf{\tilde{z}} = \{\mathbf{\tilde{z}}_1, ..., \mathbf{\tilde{z}}_n\}$, with $\mathbf{\tilde{z}}_i = (\mathbf{\tilde{z}}_{i1}, ..., \mathbf{\tilde{z}}_{iK})$. If $x_i$ is assigned to the $kth$ cluster, $\mathbf{\tilde{z}}_{ik} = 1$ else $0$. 

There are two commonly used maximum likelihood approches to attain this goal. The classification approach and mixture approach (for more details see rmixmod2 or celeux and govert 1992). The classification approach consist of considering $z$ as another unknown prameter and estimating it with $\Theta$. In this case, the parmeters are approximated using a variation of the EM algorithm called *CEM algorithm*. In **mixVarClust**, I am using the mixture approach. In this approch, the labels $z$ are identified by using the $MAP$ (Maximum A Posteriori) principle. Let $z_i$ be the label of $x_i$, $\hat{\Theta} = (\hat{p_k},\hat{\theta_k})$ the estimate of $\Theta = (p_k, \theta_k)$ the MAP is as follows : 
$$
\tilde{z}_{i k}=\left\{\begin{array}{l}
1 \text { if } k=\arg \max _{\ell=1 \ldots, K} t_{\ell}\left(\mathbf{x}_{i} \mid \hat{\Theta}\right) \\
0 \text { if not }
\end{array}\right.
$$
where : 
$$
t_{k}\left(\mathbf{x}_{i} \mid \hat{\Theta}\right)=\frac{\hat{p}_{k}f_k\left(\mathbf{x}_{i} \mid \hat{\boldsymbol{\theta}}_{k}\right)}{\sum_{\ell=1}^{K} \hat{p}_{\ell} f_{\ell}\left(\mathbf{x}_{i} \mid \hat{\boldsymbol{\theta}}_{\ell}\right)},
$$
and $\hat{\Theta}$ is the estimates found using the EM algorithm. $t_{k}\left(\mathbf{x}_{i} \mid \hat{\Theta}\right)$ is the probability of having the observation $x_i$ in the $k$th given that $\Theta$ is true (as one can see, it is the bayes theorem). The MAP assumes that $x_i$ belong to the group $k$ with the highest $t_{ik}$. 


In *mixVarClust*, I implemented 3 specific mixture models with their estimation by the EM algorithm. These models are the gaussian, the multinomial and the mix-futures mixture models. They are presented below. 

# The gaussian mixture model
In the gaussian mixture model, we suppose that each group in the dataset is distributed according to the gaussian distribution. In this case, all the futures in $X = \{x_{i1}...x_{id}\}$ are continuous variables.  The parameters of the model are $\Theta = (p_k, \theta_k) \text{ where } \theta_k = (\mu_k, \Sigma_k)$. The marginal distribution of an observation $x_i$ (or the full gaussian mixture)  is defined as: 

$$
f_{\mathbf{X}}(\mathbf{x_i}) = \sum_{k=1}^{K}p_k f_k(\mathbf{x_i}|\mu_k, \Sigma_k) 
$$
where the density of each component is the multivariate normal distribution :

\begin{equation}
f_k(x_i|\mu_k, \Sigma_k) = \frac{1}{(2 \pi)^{p / 2}\left|\Sigma_{k}\right|^{1 / 2}} \exp\left\{-\frac{1}{2}\left(\mathbf{x_i}-\mu_{k}\right)^{t} \Sigma_{k}^{-1}\left(\mathbf{x_i}-\mu_{k}\right)\right\}
\label{eq:gauss}
\end{equation}

 In the presence of multiple futures in $X$, $\Sigma_k$ is a $d\times d$ matrix variance covariance and $\mu_k$ is a vector of length $d$. If $d = 1$, i.e one future, $f_k$ is parameterize by the mean and the variance. 

## Maximum likelihood estimation 
For gaussian mixture models, the incomplete log-likelihood (equation $\ref{incomplik}$) is :
$$ 
\ell\left(\Theta; x_{i}\right)=\sum_{i=1}^{N} \log \left(\sum_{k=1}^{K} p_{k}  \frac{1}{(2 \pi)^{p / 2}\left|\Sigma_{k}\right|^{1 / 2}} \exp \left\{-\frac{1}{2}\left(\mathbf{x_i}-\mu_{k}\right)^{t} \Sigma_{k}^{-1}\left(\mathbf{x_i}-\mu_{k}\right)\right\}
\right)
$$

**Using the EM algorithm in gaussian mixtures**
| Start with arbitrary estimates of the parameters for each component $\Theta^0 = (\hat{p}_{k}, \hat{\mu}_{k}, \hat{\Sigma}_{k})$, then alternate the computation of the E step and M step.

|    *E step* : 

   * Compute the MAP $t_{k}^{q}\left(\mathbf{x}_{i} \mid \Theta^{q}\right)$, using $\Theta^0$ at the first iteration of the algorithm, use
   $\Theta^q$ for the next iterations. 
   
   $$t_{ik}^{q} = \frac{\hat{p}_k f_k(\mathbf{x_i}|\hat{\mu}_{k}, \hat{\Sigma}_{k}) }{\sum_{\ell=1}^{K}\hat{p}_\ell f_\ell(\mathbf{x_i}|\hat{\mu}_{\ell}, \hat{\Sigma}_{\ell}) } $$
   
|    *M step* : 

   * In this step we compute $\Theta^{q+1} = (\hat{p}_{k}, \hat{\mu}_{k}, \hat{\Sigma}_{k})$. These estimates are obtained by maximizing the expectation of the complete maximum likelihood $Q(\Theta, \Theta^q)$ : 
$$
Q\left(\Theta, \Theta^{q}\right)=\sum_i^N \sum_k^K t_{ik}^{q}\log \left(p_{k} \frac{1}{(2 \pi)^{p / 2}\left|\Sigma_{k}\right|^{1 / 2}} \exp \left\{-\frac{1}{2}\left(\mathbf{x_i}-\mu_{k}\right)^{t} \Sigma_{k}^{-1}\left(\mathbf{x_i}-\mu_{k}\right)\right\}\right)
$$
Subject to the constraint $\sum_k^Kp_k=1$. By using the lagrangian multipliers, we can show that, the derived estimates to compute in the M step is equal to: 
$$
\begin{array}{l}
\hat{p}_{k}=\frac{n_{k}^{q}}{n} \text { with } n_{k}=\sum_{i=1}^{n} t_{ik}^{q} \\
\hat{\mu}_{k}=\frac{1}{n_{k}^{q}} \sum_{i=1}^{n} t_{ik}^{q} \mathbf{x}_{i} \\
\hat{\Sigma}_{k}=\frac{1}{n_{k}^{q}} \sum_{i=1}^{n} t_{ik}^{q}\left(\mathbf{x}_{i}-\mu_{k}\right)^{t}\left(\mathbf{x}_{i}-\mu_{k}\right)
\end{array}
$$
 
 
  * Update the parameters : 
    
$$\Theta^q = \Theta^{q+1}$$
  
  * Evaluatre the incomplete log-likelihood and check for it convergence 
  
  $$|\ell(\Theta^{q+1},x_i) -\ell(\Theta^{q} x_i)| < \epsilon,$$
  or stop the algorithm using another stopping rule. 
  
## Three different gaussian models are implemented in mixVarClust
Fourteen different models can be derived from the gaussian mixture models by doing an eigenvalue decomposition of $\Sigma_k$, and allowing free proportions $p_k$ or restricting to equal proportions.
$$\Sigma_{k} = \lambda_kD_kA_kD_k^{t}, $$
where :  $\lambda_k = |\Sigma_{k}|^{\frac{1}{2}}$; $D_k$ is the matrix of eigenvectors of $Σ_k$. $A_k$ is a diagonal matrix, containing the normalized eigenvalues of $\Sigma_k$ in a decreasing order. 

Theses models are categorized in 3 families. The first one is called the *general family*, it include 8 models. The second family, more parsimonious than the first, is called the *diagonal family*, it include 4 models. The last family, even more parsimonious, is called the *spherical family*. For more details see...

In *mixVarClust* I implemented 3  out of the fourteen models with free proportions, including 1 model in each family. The computation of the variance covariance matrix of the implemented models in the M Step are described below:

* The model in the general family, it consider the full variance covariance matrix $\hat{\Sigma}_{k} = \lambda_kD_kA_kD_k^{t}$, in the M step, we compute : 
$$\hat{\Sigma}_{k}=\frac{1}{n_{k}} W_k,$$
where : 
$$W_k = t_{k}(x_i|\Theta)\sum_{i=1}^{n}\left(\mathbf{x}_{i}-\mu_{k}\right)^{t}\left(\mathbf{x}_{i}-\mu_{k} \right)$$


* The model in the diagonal family is parsimonious, it assumes that the covariances of all the variables is equal to 0. The variance covariance matrix is  $\hat{\Sigma}_{k} = \lambda_kB_k$: 

$$
B_{k}=\frac{\operatorname{diag}\left(W_{k}\right)}{\left|\operatorname{diag}\left(W_{k}\right)\right|^{\frac{1}{d}}}, \quad \lambda_{k}=\frac{\left|\operatorname{diag}\left(W_{k}\right)\right|^{\frac{1}{d}}}{n_{k}}
$$

* The model in the spherical family $\Sigma_k = \lambda_kI$, is extremely parsimonious, it assumes same variance for all the futures in each group with 0 covariance: 

$$
\lambda_{k}=\frac{\operatorname{tr}\left(W_{k}\right)}{d n_{k}}
$$
  
# The multinomial mixture model
**The observations to be clustered**  
The data is composed by only categorical variables $X = \{x_1,...x_j..., x_d\}$  where $d$ is the number of columns and $x_j = \{x_{j1},...x_{in} \}$,  contains $n$ observations (rows). Each categorical variable $x_j$ has $m_j$ number of categories, so there is $m_1,..., m_d$ categories for $d$ variables, and the total number of categories in the dataset is $m = \sum_{j=1}^dm_j$. 

**Binary representation of the data**  
The categorical data is represented by $n$ binary vectors $x_i= (x_{i}^{jh}; j =1,...,d; h = 1, ...,m_j)$. $x_{i}^{jh} = 1$ if  the  the observation $i$ is of the category $h$ for in the feature $j$. In another words, a one hot encoding is performed for each feature to obtain a new dataset with $n$ rows and $m_j$ columns, filled by 0 and 1. 

**The model**  
In the multinomial mixture model, we assume that the distribution of each observation $x_i$ is a mixture of multinomial distributions. For computational reasons, usually make the hypothesis that the categorical variables are independent conditionally to the latent class $z$. The formulation of the marginal distribution of $x_i$ is as follows :
$$f_X(x_i) = \sum_{k=1}^{K}p_kf_k(x_i| \alpha_{k}^{jh})$$
and the multinomial distribution of $x_i$ in each component of the mixture is defined as : 
$$
\begin{equation}
f_k(x_i| \alpha_{k}^{jh})= \prod_{j=1}^{d} \prod_{h=1}^{m_{j}}\left(\alpha_{k}^{j h}\right)^{x_{i}^{jh}}
\label{multdist}
\end{equation}
$$
With $\mathbf{\alpha_k} = (\alpha_{k}^{jh}; j =1,...,d; h = 1, ...,m_j)$. $\alpha_{k}^{jh}$ is the probability of having the category *h* in the column $j$, in group $k$. 

### The EM algorithm for multinomial mixtures
| Initialize the values  of the estimates of the parameters $\Theta_k^0 = (\hat{p}_k, \hat{\alpha}_{k}^{jh})$, then alternate the expectation step and maximization step until convergence.  
|    *E step*:  
* Compute the Maximum A Posteriori $t_{k}^q(\mathbf{x}_i|\Theta_k^{q})$ by using $θ^0$ at the first iteration of the algorithm, use $θ^q$ for the next iterations. 

$$t_{k}^{q}\left(\mathbf{x}_{i} \mid \Theta^{q}\right) = \frac{\hat{p}_k f_k(\mathbf{x_i}|\hat{\alpha}_{k}^{jh}) }{\sum_{\ell=1}^{K}\hat{p}_\ell f_\ell(\mathbf{x_i}|\hat{\alpha}_{\ell}^{jh}) } $$

|    *M step*:  
* Derived the estimates $\hat{\alpha}_{k}^{jh}$ by performing the maximization of the expected log likelihood : 
  
  $$Q\left(\Theta, \Theta^{q}\right)=\sum_i^n \sum_k^K t_{k}^{q}\left(\mathbf{x}_{i}\right)\log \left(p_{k} \prod_{j=1}^{d} \prod_{h=1}^{m_{j}}\left(\alpha_{k}^{j h}\right)^{x_{i}^{jh}}\right), $$
  subject to the constraints $\sum_k^Kp_k=1 $ and $\sum_h\alpha_k^{jh} = 1$. The lagragian helps us find the following estimates:
  $$\hat{\alpha}_{k}^{jh} = \frac{1}{n} \sum_{i = 1}^{n} t_k^q(\mathbf{x_{i}}|\Theta^q) \mathbf{x_{i}^{jh}},$$
* Update the parameters: $$\Theta^q = \Theta^{q+1}$$
* Evaluatre the incomplete log-likelihood and check for it convergence (Eq. $\ref{incomplik}$):

 $$|\ell(\Theta^{q+1},x_i) -\ell(\Theta^{q}, x_i)| < \epsilon, $$
 or stop the algorithm using another stopping rule. 
 
 In (), shows that five different models can derived. In *mixVarClust*, only one model is implemented, by computing the estimates $\hat{\alpha}_k$ as described in the M step above.  

# The Gaussian-Multinomial mixture model
In this model we consider a dataset $X$ composed by $d_q$ quantitative variables and $d_c$ categorical variables, $X = \{ \}$
In this model we assume that : 

+ the quantitative and categorical variables are independent conditionally to  latent class $Z$, 
+ the categorical variables are independent conditionally to the $Z$ and distributed according to the multinomial distribution, 
+ the continuous variables are distributed according the multivariate gaussian distribution. 

The marginal distribution of the mixture is formulated as : 
 $$f_X(x_i|\Theta) = \sum_{k=1}^{K}p_kf_k(x_i| \alpha_{k}^{jh}, \mu_k, \Sigma_k),$$
were $$f_k(x_i) = \prod_{j=1}^{d} \prod_{h=1}^{m_{j}}\left(\alpha_{k}^{j h}\right)^{x_{i}^{jh}}\times \frac{1}{(2 \pi)^{p / 2}\left|\Sigma_{k}\right|^{1 / 2}}\exp\Big(-\frac{1}{2}\left(\mathbf{x_i}-\mu_{k}\right)^{t}\Sigma_{k}^{-1}\left(\mathbf{x_i}-\mu_{k}\right)\Big). $$

### EM algorithm for Gaussian-Multinomial mixture model
| Choose arbitrary initial parameters values $\Theta_k^0 = (\hat{p}_{k}, \alpha_{k}^{jh}, \hat{\mu}_{k}, \hat{\Sigma}_{k})$, then alternate the Expectation step and Maximization step until convergence of the incomplete likelihood.
|    *E step*:  
* We compute the MAP $t_{k}^{q}\left(\mathbf{x}_{i} \mid \Theta^{q}\right)$, using $\Theta^o$ in the first iteration of the algorithm and $\Theta^q$ for the next iterations. 
   
$$t_{k}^{q}\left(\mathbf{x}_{i} \mid \Theta^{q}\right) = \frac{\hat{p}_k f_k(\mathbf{x_i}|\hat{\alpha}_{k}^{jh}, \hat{\mu}_{k}, \hat{\Sigma}_{k})}{\sum_{\ell=1}^{K}\hat{p}_\ell f_\ell(\mathbf{x_i}|\hat{\alpha}_{\ell}^{jh},\hat{\mu}_\ell, \hat{\Sigma}_\ell) } $$

|    *M step*:  
* Maximize $Q(\Theta, \Theta^q)$ with respect to the constraints $\sum_k^Kp_k = 1 \text{ and } \sum_h^{m_j}\alpha_k^{jh} = 1$, then compute the derived estimates.
   
   $$
	Q\left(\Theta, \Theta^{q}\right)=\sum_i^N \sum_k^K t_{k}^{q}\left(\mathbf{x}_{i}\right) \log \left[p_{k} \times \prod_{j=1}^{d} \prod_{h=1}^{m_{j}}\left(\alpha_{k}^{j h}\right)^{x_{i}^{jh}}\times \frac{1}{(2 \pi)^{p / 2}\left|\Sigma_{k}\right|^{1 / 2}} \exp\Big(-\frac{1}{2}\left(\mathbf{x_i}-\mu_{k}\right)^{t}\Sigma_{k}^{-1}\left(\mathbf{x_i}-\mu_{k}\right)\Big)
	\right]$$
	We can show that the derivation of $Q$ with respect to the parameters gives us the following estimates :
	
$$
\begin{array}{l}
\hat{p}_{k}=\frac{n_{k}^{q}}{n} \text { with } n_{k}=\sum_{i=1}^{n} t_{k}^{q}\left(\mathbf{x}_{i}\right) \\
\hat{\mu}_{k}=\frac{1}{n_{k}^{q}} \sum_{i=1}^{n} t_{k}^{q}\left(\mathbf{x}_{i}\right) \mathbf{x}_{i} \\
\hat{\Sigma}_{k}=\frac{1}{n_{k}^{q}} \sum_{i=1}^{n} t_{k}^{q}\left(\mathbf{x}_{i}\right)\left(\mathbf{x}_{i}-\mu_{k}\right)^{t}\left(\mathbf{x}_{i}-\mu_{k}\right)\\
\hat{\alpha}_{k}^{jh} = \frac{1}{N} \sum_{i = 1}^{N} t_k^q(\mathbf{x_{i}}|\Theta^q) \mathbf{x_{i}^{jh}}
\end{array}
$$

* Update the parameters: 
$$\Theta^q = \Theta^{q+1}$$
* Compute and check for convergence of the incomplete log-likelihood (Eq.):
 $$|\ell(\Theta^{q+1},x_i) -\ell(\Theta^{q}, x_i)| < \epsilon$$


# References

1. MIXMOD Statistical Documentation http://www.mixmod.org/IMG/pdf/statdoc_2016.pdf
2. Bouveyron C, Celeux G, Murphy T, Raftery A (2019) *Model-based clustering and classification for data science: with applications in R*. Cambridge University Press, Cambridge
